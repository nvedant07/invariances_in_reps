{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6df5c715",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/NS/twitter_archive2/work/vnanda/minconda3/envs/dl_base/lib/python3.7/site-packages/paramiko/transport.py:236: CryptographyDeprecationWarning: Blowfish has been deprecated\n",
      "  \"class\": algorithms.Blowfish,\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'dataset_metadata'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2621243/633233496.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtraining\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLitProgressBar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNicerModelCheckpointing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinetuning\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mft\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0marchitectures\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0march\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0marchitectures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLightningWrapper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLinearEvalWrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/NS/robustness_2/work/vnanda/invariances_in_reps/deep-learning-base/training/finetuning.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpytorch_lightning\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlightning\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLightningModule\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartial_inference_layer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPartialLinear\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0marchitectures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLightningWrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mOPTIMIZERS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_construct_opt_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_cosine_schedule_with_warmup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/NS/robustness_2/work/vnanda/invariances_in_reps/deep-learning-base/architectures/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcifar_models\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m### all paths are added in __init__.py of the top level dir which allows for the following import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mdataset_metadata\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0m_logger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetLogger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'dataset_metadata'"
     ]
    }
   ],
   "source": [
    "from pytorch_lightning import utilities as pl_utils\n",
    "from pytorch_lightning.trainer.trainer import Trainer\n",
    "from pytorch_lightning.plugins import DDPPlugin\n",
    "import torch\n",
    "import pathlib\n",
    "from functools import partial\n",
    "import sys, glob, os, copy\n",
    "import numpy as np\n",
    "\n",
    "sys.path.append('..')\n",
    "sys.path.append('../deep-learning-base')\n",
    "sys.path.append('../deep-learning-base/training')\n",
    "sys.path.append('../partially_inverted_reps')\n",
    "\n",
    "import plot_helper as plt_hp\n",
    "import output as out\n",
    "from training import LitProgressBar, NicerModelCheckpointing\n",
    "import training.finetuning as ft\n",
    "import architectures as arch\n",
    "from architectures.callbacks import LightningWrapper, LinearEvalWrapper\n",
    "from attack.callbacks import AdvCallback\n",
    "from datasets.data_modules import DATA_MODULES\n",
    "import datasets.dataset_metadata as dsmd\n",
    "from partial_loss import PartialInversionLoss, PartialInversionRegularizedLoss\n",
    "from __init__ import DATA_PATH_IMAGENET, DATA_PATH, SERVER_PROJECT_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5495b002",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT_PATHS = {\n",
    "    'resnet50': {\n",
    "        'nonrob': '',\n",
    "        'robustl2eps3': '/NS/robustness_3/work/vnanda/adv-robustness/logs/'\\\n",
    "                        'robust_imagenet/eps3/resnet-50-l2-eps3.ckpt'},\n",
    "    'resnet50_ff2048': {\n",
    "        'mrl0_nonrob': '/NS/robustness_2/work/vnanda/invariances_in_reps/'\n",
    "                  'deep-learning-base/checkpoints/imagenet1k/r50_mrl0_e0_ff2048.pt'},\n",
    "    'resnet50_mrl': {\n",
    "        'nonrob': '/NS/robustness_2/work/vnanda/invariances_in_reps/deep-learning-base/checkpoints'\\\n",
    "                  '/imagenet1k/r50_mrl1_e0_ff2048.pt'\n",
    "    },\n",
    "    'vit_small_patch16_224': {\n",
    "        'nonrob': \\\n",
    "            '/NS/robustness_2/work/vnanda/invariances_in_reps/deep-learning-base/checkpoints/imagenet21k/'\\\n",
    "            'S_16-i21k-300ep-lr_0.001-aug_light1-wd_0.03-do_0.0-sd_0.0.npz'\n",
    "    },\n",
    "    'vit_small_patch32_224': {\n",
    "        'nonrob': \\\n",
    "            '/NS/robustness_2/work/vnanda/invariances_in_reps/deep-learning-base/checkpoints/imagenet21k/'\\\n",
    "            'S_32-i21k-300ep-lr_0.001-aug_light1-wd_0.03-do_0.0-sd_0.0.npz'\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "11882de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 2\n",
    "NUM_NODES = 1\n",
    "DEVICES = 1\n",
    "BASE_DIR = f\"{pathlib.Path('.').parent.resolve()}/checkpoints\"\n",
    "\n",
    "FINETUNING_DATASETS = ['cifar10', 'cifar100', 'flowers', 'oxford-iiit-pets']\n",
    "FINETUNE_BS = 256\n",
    "EVAL_BATCH_SIZE = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "47a7f7a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_predictions(model_path, out):\n",
    "    preds_path = f'{model_path.split(\".ckpt\")[0]}.pred'\n",
    "    torch.save({'pred': out[0].detach().cpu(), 'gt': out[2].detach().cpu()}, preds_path)\n",
    "\n",
    "def load_predictions(model_path):\n",
    "    preds_path = f'{model_path.split(\".ckpt\")[0]}.pred'\n",
    "    if os.path.exists(preds_path):\n",
    "        return torch.load(preds_path)\n",
    "\n",
    "def accuracy(gt, pred):\n",
    "    pred = torch.argmax(pred, 1)\n",
    "    return torch.sum(gt == pred) / len(gt)\n",
    "\n",
    "\n",
    "def get_test_acc(model, source_dataset, finetuning_dataset, checkpoint_path, model_path, seed, fraction):\n",
    "    state_dict = torch.load(model_path)\n",
    "    dm = DATA_MODULES[finetuning_dataset](\n",
    "        data_dir=DATA_PATH_IMAGENET if 'imagenet' in finetuning_dataset else DATA_PATH,\n",
    "        transform_train=dsmd.TRAIN_TRANSFORMS_TRANSFER_DEFAULT(224),\n",
    "        transform_test=dsmd.TEST_TRANSFORMS_DEFAULT(224),\n",
    "        batch_size=EVAL_BATCH_SIZE)\n",
    "    dm.init_remaining_attrs(source_dataset)\n",
    "\n",
    "    ## assign mean and std from source dataset\n",
    "    m1 = arch.create_model(model, source_dataset, pretrained=True,\n",
    "                           checkpoint_path=checkpoint_path, seed=SEED, \n",
    "                           num_classes=dsmd.DATASET_PARAMS[source_dataset]['num_classes'],\n",
    "                           callback=partial(LightningWrapper, \n",
    "                                            dataset_name=source_dataset,\n",
    "                                            inference_kwargs={'with_latent': True}),\n",
    "                           loading_function_kwargs={'strict': False})\n",
    "    new_layer = ft.setup_model_for_finetuning(\n",
    "        m1.model, \n",
    "        dsmd.DATASET_PARAMS[finetuning_dataset]['num_classes'],\n",
    "        FINETUNE_MODE, fraction, seed)\n",
    "    print (new_layer.__dict__)\n",
    "    linear_layer = list(m1.model.named_modules())[-1][1]\n",
    "    linear_layer.load_state_dict({k.split('.')[-1]:v \\\n",
    "                                  for k,v in state_dict['state_dict'].items()}, strict=True)\n",
    "    if hasattr(new_layer, 'neuron_indices') and 'neuron_indices' in state_dict:\n",
    "        assert torch.all(new_layer.neuron_indices == state_dict['neuron_indices'])\n",
    "    pl_utils.seed.seed_everything(seed, workers=True)\n",
    "\n",
    "    trainer = Trainer(accelerator='gpu', \n",
    "                      devices=DEVICES,\n",
    "                      num_nodes=NUM_NODES,\n",
    "                      log_every_n_steps=1,\n",
    "                      auto_select_gpus=True, \n",
    "                      deterministic=True,\n",
    "                      check_val_every_n_epoch=1,\n",
    "                      num_sanity_val_steps=0,\n",
    "                      callbacks=[\n",
    "                        LitProgressBar(['loss', \n",
    "                                        'running_test_acc'])])\n",
    "\n",
    "    out = trainer.predict(m1, dataloaders=[dm.test_dataloader()])\n",
    "    save_predictions(model_path, out)\n",
    "    gt, pred = out[2], out[0]\n",
    "    return accuracy(gt, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0e2430",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
